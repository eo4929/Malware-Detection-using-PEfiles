
# 마지막에, sns.heatmap(rf_confusion_matrix) 로 결과 확인하기

import pandas as pd
import numpy as np

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

from pycaret.classification import *
from pycaret.utils import check_metric

class ModelMaker:
    def __init__(self):
        self.raw_data = None
        self.pd_data_list = None
        self.data_list = [] # labeled data list

        self.X_train_list = []
        self.X_test_list = []
        self.y_train_list = []
        self.y_test_list = []

        self.final_model_list = []

    def load_data(self, cleaned_data_list):
        self.raw_data = pd.read_csv('C:/Users/Dae-Young Park/Desktop/AI스터디/연습용데이터셋/dataset_PEmalwares.csv')
        self.pd_data_list = cleaned_data_list

    def prepare_labeled_data_list(self):
        col_label = self.raw_data['Malware']

        for each_scaled_data in self.pd_data_list:
            df_each_scaled_data = pd.DataFrame(each_scaled_data)
            res = pd.concat([df_each_scaled_data, col_label], axis=1)

            shuffled_res = shuffle(res)
            self.data_list.append(shuffled_res)

    def split_data(self):
        for each_scaled_data in self.data_list:
            y = each_scaled_data['Malware']
            X = each_scaled_data.drop('Malware', axis=1)
            print(X)
            print(y)
            X_train, X_test, y_train, y_test = train_test_split(X,y, shuffle=True, test_size=0.25, stratify=y)

            self.X_train_list.append(X_train)
            self.X_test_list.append(X_test)
            self.y_train_list.append(y_train)
            self.y_test_list.append(y_test)

    def prepare_model_list(self):
        '''
        :return: scaled data 마다 최적의 model으로 구성된 model list
        '''
        for idx in range(len(self.data_list)):
            print('- - - - - - - - - - - - - - - - ')
            print('각 scaled data 를 통해 모델 구축 시작 (idx: {})'.format(idx))
            print('- - - - - - - - - - - - - - - - ')

            each_training_data = pd.concat([self.X_train_list[idx], self.y_train_list[idx]], axis=1)

            s = setup(each_training_data, target='Malware', train_size=0.7,
                      fold_strategy='stratifiedkfold', n_jobs=2, fix_imbalance=True,
                      feature_selection=True, feature_interaction=True,
                      feature_selection_threshold=0.7, interaction_threshold=0.01)

            models_top3 = compare_models(whitelist = models(type='ensemble').index.tolist(),
                                         fold=5, sort='F1', n_select=3, round=4, probability_threshold=0.5)

            print('models_top3: ')
            print()
            print(models_top3)
            print()

            tuned_models_top3 = [ tune_model(each_model, n_iter=15, optimize='F1', search_library='optuna', search_algorithm='tpe',
                                             choose_better=True)
                                  for each_model in models_top3 ]

            print('tuned_models_top3: ')
            print()
            print(tuned_models_top3)
            print()

            ensemble_models_top3_Bagging = [ ensemble_model(each_tuned_model, method='Bagging', n_estimators=15,
                                                            choose_better=True, optimize='F1', probability_threshold=0.5)
                                     for each_tuned_model in tuned_models_top3]
            ensemble_models_top3_Boosting = [ensemble_model(each_tuned_model, method='Boosting', n_estimators=15,
                                                           choose_better=True, optimize='F1', probability_threshold=0.5)
                                            for each_tuned_model in tuned_models_top3]
            #ensemble_models_top3 = ensemble_models_top3_Bagging + ensemble_models_top3_Boosting

            print('ensemble_models_top3_Bagging: ')
            print()
            print(ensemble_models_top3_Bagging)
            print()
            print('ensemble_models_top3_Boosting: ')
            print()
            print(ensemble_models_top3_Boosting)
            print()

            blender_models_top3_Bagging = blend_models(estimator_list=ensemble_models_top3_Bagging,
                                                       optimize='F1', method='auto', choose_better=True, probability_threshold=0.5)
            blender_models_top3_Boosting = blend_models(estimator_list=ensemble_models_top3_Boosting,
                                                       optimize='F1', method='auto', choose_better=True,
                                                       probability_threshold=0.5)

            print('blender_models_top3_Bagging: ')
            print()
            print(blender_models_top3_Bagging)
            print()
            print('blender_models_top3_Boosting: ')
            print()
            print(blender_models_top3_Boosting)
            print()

            total_models = tuned_models_top3 + blender_models_top3_Bagging + blender_models_top3_Boosting
            
            stacker = stack_models( [tuned_models_top3, blender_models_top3_Bagging, blender_models_top3_Boosting],
                                    meta_model=None, method='auto', choose_better=True,optimize='F1', probability_threshold=0.5)

            print('stacker: ')
            print()
            print(stacker)
            print()

            final_model = finalize_model(stacker)
            self.final_model_list.append(final_model)

            print('- - - - - - - - - - - - - - - - ')
            print('각 scaled data 를 통해 모델 구축 완료 (한 final model 생성)')
            print('- - - - - - - - - - - - - - - - ')

    def predict_and_evaluate(self):
        for idx in range(len(self.final_model_list)):
            print('- - - - - - - - - - - - - - - - ')
            print('각 final 모델에 대한 성능 평가')
            print('- - - - - - - - - - - - - - - - ')

            prediction_result = predict_model(self.final_model_list[idx], data=self.X_test_list[idx])

            eval_f1 = check_metric( self.y_test_list[idx], prediction_result['Label'] ,metric='F1')
            print(eval_f1)
            eval_prec = check_metric(self.y_test_list[idx], prediction_result['Label'], metric='precision')
            print(eval_prec)
            eval_re = check_metric(self.y_test_list[idx], prediction_result['Label'], metric='recall')
            print(eval_re)

            print()
            print()
            


class SingleModelMaker: 
    def __init__(self):
        self.raw_data = None
        self.pd_data = None

        self.data = None # labeled data

        self.X_train=None
        self.X_test=None
        self.y_train=None
        self.y_test=None

        self.final_model=None

    def load_data(self, cleaned_data):
        self.raw_data = pd.read_csv('C:/Users/Dae-Young Park/Desktop/AI스터디/연습용데이터셋/dataset_PEmalwares.csv')
        self.pd_data = cleaned_data # standardized data 활용

    def prepare_labeled_data(self):
        col_label = self.raw_data['Malware']

        df_pd_data = pd.DataFrame(self.pd_data)
        res = pd.concat([df_pd_data, col_label], axis=1)
        shuffled_res = shuffle(res)
        self.data = shuffled_res

    def split_data(self):
        y = self.data['Malware']
        X = self.data.drop('Malware', axis=1)

        X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.25, stratify=y)

        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test

    def prepare_model_list(self):
        for idx in range(len(self.data_list)):
            print('- - - - - - - - - - - - - - - - ')
            print('각 scaled data 를 통해 모델 구축 시작 (idx: {})'.format(idx))
            print('- - - - - - - - - - - - - - - - ')

            each_training_data = pd.concat([self.X_train_list[idx], self.y_train_list[idx]], axis=1)

            s = setup(each_training_data, target='Malware', train_size=0.7,
                      fold_strategy='stratifiedkfold', n_jobs=2, fix_imbalance=True,
                      feature_selection=True, feature_interaction=True,
                      feature_selection_threshold=0.7, interaction_threshold=0.01)

            rf = create_model(estimator='rf', probability_threshold=0.5, fold=5)

            print('rf: ')
            print()
            print(rf)
            print()

            #tuned_rf = tune_model(rf, n_iter=15, optimize='F1', search_library='optuna', search_algorithm='tpe', choose_better=True)
            tuned_rf = tune_model(rf, n_iter=10, optimize='F1', search_library='scikit-learn', search_algorithm='random', choose_better=True)

            print('tuned_rf: ')
            print()
            print(tuned_rf)
            print()

            tuned_rf_Bagging = ensemble_model(tuned_rf, method='Bagging', n_estimators=15,
                                                           choose_better=True, optimize='F1', probability_threshold=0.5)

            tuned_rf_Boosting = ensemble_model(tuned_rf, method='Boosting', n_estimators=15,
                                                            choose_better=True, optimize='F1',
                                                            probability_threshold=0.5)
            # ensemble_models_top3 = ensemble_models_top3_Bagging + ensemble_models_top3_Boosting

            print('tuned_rf_Bagging: ')
            print()
            print(tuned_rf_Bagging)
            print()
            print('tuned_rf_Boosting: ')
            print()
            print(tuned_rf_Boosting)
            print()

            stacker = stack_models([tuned_rf, tuned_rf_Bagging, tuned_rf_Boosting],
                                   meta_model=None, method='auto', choose_better=True, optimize='F1',
                                   probability_threshold=0.5)

            print('stacker: ')
            print()
            print(stacker)
            print()

            final_model = finalize_model(stacker)
            self.final_model_list.append(final_model)

            print('- - - - - - - - - - - - - - - - ')
            print('각 scaled data 를 통해 모델 구축 완료 (한 final model 생성)')
            print('- - - - - - - - - - - - - - - - ')

    def prepare_model_simple_v(self):
        print('- - - - - - - - - - - - - - - - ')
        print('rescaled data 를 통해 모델 구축 시작')
        print('- - - - - - - - - - - - - - - - ')

        training_data = pd.concat([self.X_train, self.y_train], axis=1)

        s = setup(training_data, target='Malware', train_size=0.7,
                  fold_strategy='stratifiedkfold', n_jobs=2, fix_imbalance=True,
                  feature_selection=True, feature_interaction=True,
                  feature_selection_threshold=0.7, interaction_threshold=0.01)

        rf = create_model(estimator='rf', probability_threshold=0.5, fold=5)

        print('rf: ')
        print()
        print(rf)
        print()

        #tuned_rf = tune_model(rf, n_iter=5, optimize='F1', search_library='scikit-learn', search_algorithm='random', choose_better=True)
        tuned_rf = tune_model(rf, n_iter=5, optimize='F1', search_library='optuna', search_algorithm='tpe', choose_better=True)

        print('tuned_rf: ')
        print()
        print(tuned_rf)
        print()

        stacker = stack_models([rf, tuned_rf],
                               meta_model=None, method='auto', choose_better=True, optimize='F1',
                               probability_threshold=0.5)

        print('stacker: ')
        print()
        print(stacker)
        print()

        final_model = finalize_model(stacker)
        self.final_model = final_model

        print('- - - - - - - - - - - - - - - - ')
        print('rescaled data 를 통해 모델 구축 완료 (한 final model 생성)')
        print('- - - - - - - - - - - - - - - - ')

    def predict_and_evaluate(self):
        print('- - - - - - - - - - - - - - - - ')
        print('final 모델에 대한 성능 평가')
        print('- - - - - - - - - - - - - - - - ')

        prediction_result = predict_model(self.final_model, data=self.X_test)

        eval_f1 = check_metric( self.y_test, prediction_result['Label'] ,metric='F1')
        print('F1: ', eval_f1)
        eval_prec = check_metric(self.y_test, prediction_result['Label'], metric='Precision')
        print('Prec: ', eval_prec)
        eval_re = check_metric(self.y_test, prediction_result['Label'], metric='Recall')
        print('Recall: ', eval_re)

        print()
        print()